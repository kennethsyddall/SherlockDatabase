{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341782f9-b340-4217-87c1-3bb352f2746c",
   "metadata": {},
   "source": [
    "# [Sherlock Scraping](https://sherloc.unodc.org)\n",
    "\n",
    "Web scraching algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd8a4f0-4f6e-4cda-9ea2-34155c863ab2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - Armar la funcion para iterar los paises\n",
    "\n",
    "### - Crear un sistema de carga reanudable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bcf7cfb9-4bac-4562-84e6-e96dc834497d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import selenium as sl\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import urllib\n",
    "\n",
    "# List of Latin American countries to scrape\n",
    "\n",
    "lat_ctrs=['Antigua and Barbuda','Argentina','Bahamas','Barbados','Belize','Bolivia (Plurinational State of)','Brazil','Canada','Chile','Colombia',\n",
    "    'Costa Rica','Cuba','Dominican Republic','Ecuador','El Salvador','United States of America','Grenada',\n",
    "    'Guatemala','Guyana','Haiti','Honduras','Jamaica','Mexico','Nicaragua','Panama','Paraguay','Dominica','Peru',\n",
    "    'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Saint Lucia','Suriname','Trinidad and Tobago',\n",
    "    'Uruguay','Venezuela (Bolivarian Republic of)']\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def search_ctrs(url: str) -> list:\n",
    "    \"\"\"Function to see countries are abalible.\n",
    "    \"\"\"\n",
    "    \n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "    sleep(15)\n",
    "    if '/treaties/' in url:\n",
    "        elem=driver.find_element_by_xpath('//a[@href=\"#country-treaty\"]')\n",
    "        elem.click()\n",
    "    sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    non_ctrs = soup.find_all('a',class_=\"cover-parent\")\n",
    "    driver.quit()\n",
    "    ctrs=[]\n",
    "    for c in non_ctrs:\n",
    "        ctr =c.get(\"href\").split('\"value\":\"')[1].split('\"}]')[0]\n",
    "        ctrs.append(ctr)\n",
    "    return ctrs\n",
    "##########################################################################################\n",
    "\n",
    "def spaceend(title: str) -> str:\n",
    "    \"\"\"Function to delete extra spaces.\n",
    "    \"\"\"\n",
    "    oki=0\n",
    "    okd=0\n",
    "    while oki == 0 or okd == 0:\n",
    "\n",
    "        # Si no hay titulos los llamo None\n",
    "        if len(title) == 0 or title== ' ':\n",
    "            title = 'None'\n",
    "        \n",
    "        if title[-1] == ' ' and okd == 0:\n",
    "            title = title[:-1]\n",
    "        else:\n",
    "            okd = 1\n",
    "            \n",
    "        if title[0] == ' ' and oki == 0:\n",
    "            title = title[1:]\n",
    "        else:\n",
    "            oki = 1\n",
    "\n",
    "    return title\n",
    "    \n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "def extract_urls(ctr: str,op: str) -> (dict, int):\n",
    "    \"\"\"Function to extract titles and urls of the\n",
    "    text pages.\n",
    "    \"\"\"\n",
    "    # Create the url for different options\n",
    "    \n",
    "    if op == 'Legislation':\n",
    "        op = \"legdb\"\n",
    "        add = \"legislation@country_label_s\"\n",
    "        \n",
    "    elif op == 'Strategies':\n",
    "        op = \"strategies\"\n",
    "        add = \"strategy.country..country.name.html_s\"\n",
    "        \n",
    "    elif op == 'Treaties':\n",
    "        op = \"treaties\"\n",
    "        add = \"countryTreatyStatus.countriesInvolved.country..country.name.html_s1\"\n",
    "    \n",
    "    driver = webdriver.Firefox()\n",
    "    \n",
    "    url = 'https://sherloc.unodc.org/cld/v3/sherloc/'+op+'/search.html?lng=en#?c=%7B%22filters%22:%5B%7B%22fieldName%22:%22en%23'+add+'%22,%22value%22:%22'+ctr+'%22%7D%5D,%22sortings%22:%22%22%7D'\n",
    "    \n",
    "    # Try to connect, if not, try again\n",
    "    try_conn = 0\n",
    "    while try_conn == 0:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            try_conn = 1\n",
    "        except:\n",
    "            try_conn = 0\n",
    "            sleep(1)\n",
    "            \n",
    "    # Take the list of urls, and its compared with the number that the page say.\n",
    "    # Is compared until it matches.\n",
    "    urls = []\n",
    "    found = 10000\n",
    "    while found != len(urls):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        \n",
    "        # If there are nothing close the window\n",
    "        if op != 'legdb':\n",
    "            nothf = soup.find('div', class_=\"ng-hide\", id=\"nothing-found\")\n",
    "            if nothf == None:\n",
    "                driver.quit()\n",
    "                return {},0\n",
    "        \n",
    "        # Search the number of urls\n",
    "        try:\n",
    "            found = soup.find('div', id=\"found-count\")\n",
    "            found = int(found.find(text=True).split(' ')[1])\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # Search the urls\n",
    "        tags = soup.find_all('a',class_=\"cover-parent\")\n",
    "        \n",
    "        urls=[]\n",
    "        for tag in tags:\n",
    "            url = 'https://sherloc.unodc.org' + tag.get(\"href\")\n",
    "            \n",
    "            if 'v3' not in url: \n",
    "                urls.append(url)\n",
    "    \n",
    "    # Take the titles for every url\n",
    "    \n",
    "    if op == 'legdb':\n",
    "        cuad = str(soup.find('div',class_=\"animated fadeIn new my-3 ng-scope\"))\n",
    "        titles = titles_legis(cuad)\n",
    "        \n",
    "    elif op == 'strategies':\n",
    "        cuads = soup.find_all('div', class_=\"col-md-12 col-xs-12 result-title\")\n",
    "        titles = {}\n",
    "        for index, cuad in enumerate(cuads):\n",
    "            title=cuad.find(text=True).replace('  ',' ').replace('  ',' ')\n",
    "            title=spaceend(title)\n",
    "            title = title.replace('á','á').replace('é','é').replace('í','í').replace('ó','ó').replace('ú','ú')\n",
    "            titles[title]=[{title:urls[index]}]\n",
    "            \n",
    "    elif op == 'treaties':\n",
    "        cuad = soup.find_all('div', class_=\"panel-heading signatures-heading\")\n",
    "        titles = {}\n",
    "        for index, cu in enumerate(cuad):\n",
    "            text=cu.find_all(text=True)\n",
    "            line1=text[0].replace('\\n                            ','').replace('\\n                        ','')\n",
    "            \n",
    "            if len(text) == 4:\n",
    "                line2=text[2].replace('\\xa0','')\n",
    "                title = (line1+' '+line2).replace('  ',' ').replace('  ',' ')\n",
    "                title=spaceend(title)\n",
    "                title = title.replace('á','á').replace('é','é').replace('í','í').replace('ó','ó').replace('ú','ú')\n",
    "                titles[title]=[{title:urls[index]}]\n",
    "            else:\n",
    "                title = line1.replace('  ',' ').replace('  ',' ')\n",
    "                title=spaceend(title)\n",
    "                title = title.replace('á','á').replace('é','é').replace('í','í').replace('ó','ó').replace('ú','ú')\n",
    "                titles[title]=[{title:urls[index]}]\n",
    "                \n",
    "    driver.quit()\n",
    "    \n",
    "    return titles, len(urls)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def remplace_title_text(text: list) -> list:\n",
    "    \"\"\"Replace all the unintedible unicode characters with\n",
    "    the alternative characters to latex.\n",
    "    \"\"\"\n",
    "    \n",
    "    ctr_text={'«':'$<<$','»':'$>>$',\"_\":\"\\_\",\"%\":\"\\%\",\"&\":\"\\&\",\n",
    "              \"$\":\"\\\\\\$ \",\"\":\"•\",\"\":\"•\",\"\":\"•\",'\"':\"'\",'ñ':'ñ',\n",
    "              'ó':'ó','á':'á','é':'é','í':'í','ú':'ú','Á':'Á',\n",
    "              'É':'É','Í':'Í','Ó':'Ó','Ú':'Ú','−':'-','─':'-',\n",
    "              '«':'$\\ll$ ','»':'$\\gg$ ','ê':'ê','ô':'ô','ã':'ã',\n",
    "              'õ':'õ','à':'à','è':'è','ô':'ô','ç':'ç','â':'â',\n",
    "              'ü':'ü','#':'\\#','':'.',' ':'•',' ':'•','':'•',\n",
    "              ';':';',' ':'•','`(':'(','а':'a','ï':'ï','û':'û',\n",
    "              '}-':')-','{':'(','}':')','`S':'S','`E':'E',\n",
    "              '':'$\\rightarrow$ ','●':'•','\\\\':'',\"`,\":\"´,\",'\"':'\\\\\"'}\n",
    "    \n",
    "    \n",
    "    ctr_text = dict((re.escape(k), v) for k, v in ctr_text.items())\n",
    "    pattern_text = re.compile(\"|\".join(ctr_text.keys()))\n",
    "    \n",
    "    new_text=[]\n",
    "    for n,line in enumerate(text):\n",
    "        if text[n]=='\\n' or text[n]=='\\xa0':\n",
    "            new_text.append('')\n",
    "            new_text.append('')\n",
    "        text[n] = pattern_text.sub(lambda m: ctr_text[re.escape(m.group(0))], text[n])\n",
    "        new_text.append(text[n])\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def load_text(url: str) -> (list, str):\n",
    "    \"\"\"Searh the respective text and take the urls\n",
    "    to download the attached files.\n",
    "    \"\"\"\n",
    "    \n",
    "    try_conn=0\n",
    "    while try_conn==0:\n",
    "        try:\n",
    "            res = requests.get(url)\n",
    "            try_conn=1\n",
    "        except:\n",
    "            try_conn=0\n",
    "            sleep(1)\n",
    "            \n",
    "    html_page = res.content\n",
    "    soup = BeautifulSoup(html_page, 'lxml')\n",
    "    attach = []\n",
    "    \n",
    "    if '/legislation/' in url:\n",
    "        title, text = ('epa', ['salepa', 'quepacho'])\n",
    "        cuad = soup.find('div',class_=\"case-law col-md-8 col-lg-9 col-xs-12 topSpace10\")\n",
    "        text = cuad.find_all(text=True)\n",
    "        \n",
    "        if 'Attachments' in text:\n",
    "            for at in str(cuad.find('div',class_=\"attachments\")).split('href=\"'):\n",
    "                if '/cld/doc' in at:\n",
    "                    attach.append('https://sherloc.unodc.org' + at.split('\" ')[0])\n",
    "                    \n",
    "    elif '/treaties/status/' in url:\n",
    "        cuad = soup.find_all('div',class_=\"container\")\n",
    "        text = cuad[3].find_all(text=True)\n",
    "        if 'Attachments' in text:         # I can't find attached files for treaties\n",
    "            print(\"Attached file not downloaded. \" + title)\n",
    "            \n",
    "    elif '/strategies/' in url:\n",
    "        cuad = soup.find('div',class_=\"col-md-9 col-lg-9 col-xs-12 topSpace20\")\n",
    "        text = cuad.find_all(text=True)\n",
    "        \n",
    "        if 'Attachments' in text:\n",
    "            for at in str(cuad.find('div',class_=\"topSpace10\")).split('href=\"'):\n",
    "                if '/cld/doc' in at:\n",
    "                    attach.append('https://sherloc.unodc.org' + at.split('\" ')[0])\n",
    "    \n",
    "    text = remplace_title_text(text)\n",
    "    \n",
    "    return text, attach\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def down_attach(url: str, path: str, fold_title: str) -> (list):\n",
    "    \"\"\"To diferent tipe of url download the attached file\n",
    "    saving it in a dedicated folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cut the long names\n",
    "    if len(fold_title) < 139: \n",
    "        fold_path = path + '/' + fold_title\n",
    "        if not os.path.exists(fold_path):\n",
    "            os.makedirs(fold_path)\n",
    "    else:\n",
    "        ps= fold_title[0:135]\n",
    "        for i in range(len(ps)):\n",
    "            if ps[-1] != ' ':\n",
    "                ps = ps[:-1]\n",
    "            else:\n",
    "                fold_title = ps + '(...)'\n",
    "    \n",
    "    fold_path = path + '/' + fold_title + '/Attachments/'\n",
    "    if not os.path.exists(fold_path):\n",
    "        os.makedirs(fold_path)\n",
    "    \n",
    "    not_dw = ''\n",
    "    \n",
    "    # Si el link es de una pagina buscar el descargable, sino descargalo\n",
    "    if '.html' in url:\n",
    "        con=0\n",
    "        while con == 0: # Wait to connect to internet\n",
    "            try:\n",
    "                res = requests.get(url)\n",
    "                con = 1\n",
    "            except:\n",
    "                con = 0\n",
    "                sleep(1)\n",
    "        \n",
    "        html_page = res.content\n",
    "        soup = BeautifulSoup(html_page, 'lxml')\n",
    "        cuad = soup.find_all('div',class_=\"link\")\n",
    "        \n",
    "        if len(cuad)==0: # 404 pages\n",
    "            not_dw = 'Error 404: ' + url\n",
    "        elif len(cuad) > 1:\n",
    "            for link in cuad:\n",
    "                not_dw = not_dw + ',' + 'https://sherloc.unodc.org' + str(link).split('href=\"')[1].split('\" ')[0]\n",
    "        else:\n",
    "            attach = 'https://sherloc.unodc.org' + str(cuad[0]).split('href=\"')[1].split('\" ')[0]\n",
    "            \n",
    "    elif '.pdf' in url:\n",
    "        attach = url\n",
    "    else:\n",
    "        not_dw = url\n",
    "\n",
    "    if not_dw == '':\n",
    "        title = attach.split('/')[-1].split('.pdf')[0]\n",
    "        fold_path = fold_path + title\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(attach, stream=True)\n",
    "            with open(fold_path+'.pdf', 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        except:\n",
    "            not_dw = url\n",
    "    return not_dw\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def generate_latex_text(title: str, text: list) -> (list):\n",
    "    \"\"\"This function generates the list with all lines that will be agregated\n",
    "    to the .tex file.\"\"\"\n",
    "    \n",
    "    title = title.replace(\"_\", \"\\_\").replace(\"&\",\"\\&\").replace('\"','\\\\\"')\n",
    "    \n",
    "    latex_text=['\\\\documentclass{article}',\n",
    "                '\\\\usepackage[utf8]{inputenc}',\n",
    "                '\\\\title{'+ title +'}',\n",
    "                '\\\\author{}',\n",
    "                '\\\\date{}',\n",
    "                '\\\\begin{document}',\n",
    "                '\\\\maketitle']\n",
    "    \n",
    "    for line in text:\n",
    "        latex_text.append(line)\n",
    "    end_text='\\\\end{document}'\n",
    "    latex_text.append(end_text)\n",
    "    \n",
    "    return latex_text\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def titles_legis(cuad_str: str) -> (dict):\n",
    "    \"\"\"Search the titles in the legislation pages.\n",
    "    \"\"\"\n",
    "    # Take the part with the principal titles\n",
    "    principal_titles = cuad_str.split('<p class=\"law-title py-1 ng-binding\">')\n",
    "    vals = {}\n",
    "    for raw_title in principal_titles:\n",
    "        if '<span class=\"ng-binding ng-scope\"' in raw_title:\n",
    "            \n",
    "            # Take the title\n",
    "            title = raw_title[raw_title.index('</span>')+len('</span>'):raw_title.index('</p>')]\n",
    "            \n",
    "            title = spaceend(title[0:-33]).replace(\" \\xa0\",\"\").replace(\"&amp;\", \"&\").replace('  ',' ').replace('  ',' ')\n",
    "            title = title.replace('á','á').replace('é','é').replace('í','í').replace('ó','ó').replace('ú','ú')\n",
    "            vals[title] = []\n",
    "            \n",
    "            # Take the part with subtitles\n",
    "            raw_subtitles_split = raw_title.split('<p class=\"leg-article mx-2 py-2 ng-scope\"')\n",
    "            \n",
    "            for raw_subtitles in raw_subtitles_split:\n",
    "                elem = {}\n",
    "                \n",
    "                # Take the subtitles\n",
    "                if '<span class=\"ng-binding ng-scope\"' in raw_subtitles:\n",
    "                    subtitles = raw_subtitles.split('<span class=\"ng-binding ng-scope\"')\n",
    "                    subtitle = ''\n",
    "                    for raw_subtitle in subtitles:\n",
    "                        if '</i></span>' in raw_subtitle:\n",
    "                            subtitle = subtitle + raw_subtitle[raw_subtitle.index('</i></span>')+len('</i></span>'): raw_subtitle.index('</span><!')]\n",
    "                            \n",
    "                    subtitle = subtitle.replace(\" \\xa0\",\"\").replace(\"&amp;\", \"&\")\n",
    "                    subtitle = spaceend(subtitle).replace('  ',' ').replace('  ',' ')\n",
    "                    subtitle = subtitle.replace('á','á').replace('é','é').replace('í','í').replace('ó','ó').replace('ú','ú')\n",
    "                    \n",
    "                    link = raw_subtitles[raw_subtitles.index('href=\"')+len('href=\"'): raw_subtitles.index('\" ng-click=\"$event.stopPropagation();')]#'\"></a>')]\n",
    "                    elem[subtitle] = 'https://sherloc.unodc.org' + link\n",
    "                    \n",
    "                    vals[title].append(elem)\n",
    "    return vals\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def create_file_system(path: str, fold_title: str, latex_text:list, page_title: str, url:str) -> (list, str):\n",
    "    \"\"\"Function to create the .pdf files and his respective folder system.\n",
    "    \"\"\"\n",
    "    \n",
    "    fold_title = fold_title.replace('/','_').replace('  ',' ')\n",
    "    page_title = page_title.replace('/','_').replace('  ',' ')\n",
    "    \n",
    "    # Cambio las comillas que dan problemas\n",
    "    comillas = 0\n",
    "    if '\"' in fold_title or '\"' in page_title:\n",
    "        fold_title = fold_title.replace('\"',\"'''\")\n",
    "        page_title = page_title.replace('\"',\"'''\")\n",
    "        comillas = 1\n",
    "    \n",
    "    # Acorto los nombres\n",
    "    if len(page_title) > 140:\n",
    "        ps= page_title[0:135]\n",
    "        for i in range(len(ps)):\n",
    "            if ps[-1] != ' ':\n",
    "                ps = ps[:-1]\n",
    "            else:\n",
    "                page_title = ps + '(...)'\n",
    "    \n",
    "    if len(fold_title) < 139:\n",
    "        fold_path = path + '/' + fold_title\n",
    "        if not os.path.exists(fold_path):\n",
    "            os.makedirs(fold_path)\n",
    "    else:\n",
    "        ps= fold_title[0:135]\n",
    "        for i in range(len(ps)):\n",
    "            if ps[-1] != ' ':\n",
    "                ps = ps[:-1]\n",
    "            else:\n",
    "                fold_title = ps + '(...)'\n",
    "        \n",
    "        fold_path = path+ '/' + fold_title\n",
    "        \n",
    "        \n",
    "        if not os.path.exists(fold_path):\n",
    "            os.makedirs(fold_path)    \n",
    "    \n",
    "    \n",
    "    # Creo el .tex y luego el pdf\n",
    "    for j,line in enumerate(latex_text):\n",
    "        os.system('(echo \"' + line +'\" >> \"'+ page_title + '.tex\")')\n",
    "        \n",
    "    os.system('(pdflatex \"' + page_title + '.tex\" >/dev/null 2>&1)')\n",
    "    \n",
    "    # Espera a que se generen los archivos\n",
    "    OK = 0\n",
    "    while (OK < 10 and\n",
    "           not(os.path.exists(page_title+'.aux') and\n",
    "               os.path.exists(page_title+'.log') and\n",
    "               os.path.exists(page_title+'.pdf'))):\n",
    "        \n",
    "        sleep(1)\n",
    "        OK += 1\n",
    "        if OK == 10:\n",
    "            print('Error loading: ' + fold_title + page_title + ' '*40 +'\\n'+ url +'\\n')\n",
    "    \n",
    "    \n",
    "    if OK < 10:\n",
    "        Black_List = []\n",
    "        msg = page_title + ' OK'\n",
    "        os.system('rm \"'+page_title+'.aux\"')\n",
    "        os.system('rm \"'+page_title+'.log\"')\n",
    "        if comillas == 0:\n",
    "            os.system('mv \"'+page_title+'.pdf\" \"'+fold_path+'/'+page_title+'.pdf\"')\n",
    "        else:\n",
    "            os.system('mv \"'+page_title+'.pdf\" \"'+fold_path+'/'+page_title.replace(\"'''\",'\\\\\"')+'.pdf\"')\n",
    "        os.system('rm \"'+page_title+'.tex\"')\n",
    "        \n",
    "    elif OK == 10:\n",
    "        Black_List = [url]\n",
    "        msg = page_title + ' Fail'\n",
    "        os.system('rm \"'+page_title+'.aux\"')\n",
    "        os.system('rm \"'+page_title+'.log\"')\n",
    "        os.system('mv \"'+page_title+'.tex\" \"'+page_title+'.error.tex\"')\n",
    "    \n",
    "    return Black_List, msg\n",
    "        \n",
    "##########################################################################################\n",
    "    \n",
    "def create_fold(ctr: str, path: str) -> (list, list, list):\n",
    "    \"\"\"This function creates the first three folders for each country.\n",
    "    \"\"\"\n",
    "    \n",
    "    tipes=['Legislation','Strategies','Treaties']\n",
    "    init_path = path\n",
    "    Black_List, attachs, not_dw, msgs = [], [], [], []\n",
    "    \n",
    "    print(ctr+ ': ')\n",
    "    \n",
    "    for tipe in tipes:\n",
    "        titles, top = extract_urls(ctr,tipe)\n",
    "        \n",
    "        path = init_path + '/' + tipe\n",
    "        \n",
    "        val=0\n",
    "        for fold_title,pages in titles.items():\n",
    "            fold_at = []\n",
    "            for page in pages:\n",
    "                val+=1\n",
    "                page_title,url = list(page.items())[0]\n",
    "                text ,ats = load_text(url)\n",
    "                latex_text = generate_latex_text(page_title,text)\n",
    "                bl,msg = create_file_system(path, fold_title, latex_text, page_title, url)\n",
    "                Black_List = Black_List + bl\n",
    "                \n",
    "                if ats != []:\n",
    "                    for at in ats:\n",
    "                        if at not in fold_at:\n",
    "                            nd = down_attach(at, path, fold_title)\n",
    "                            if nd != '':\n",
    "                                not_dw.append(nd)\n",
    "                            fold_at.append(at)\n",
    "                            \n",
    "                        if at not in attachs:\n",
    "                            attachs.append(at)\n",
    "                msgs.append(msg)\n",
    "                \n",
    "                if len(msgs) > 1:\n",
    "                    print(' '*len(ctr) +' ' * len(msgs[-2])+' ' * 34,end='\\r')\n",
    "                print(' '*len(ctr) + '-' + tipe+ ' ' +str(int(val*100/top))+'% | '+ msgs[-1],end='\\r')\n",
    "        \n",
    "        print(' '*len(ctr) + '-' + tipe+ ' 100%' + ' ' * (len(msgs[-1])+4))\n",
    "    \n",
    "    return Black_List, attachs, not_dw\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def download_lat_files(path: str, lat_ctrs: list) -> (list, list, list):\n",
    "    \"\"\"This function take the of the pages for each country and stores them in files.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    loaded_ctr=os.listdir(path)\n",
    "    \n",
    "    Black_list, attach, not_down = [], [], []\n",
    "    \n",
    "    for lc in lat_ctrs:\n",
    "        if lc + '_inc' in loaded_ctr:\n",
    "            os.system('rm -rf \"'+path+lc+'_inc'+'\"')\n",
    "        \n",
    "        if not lc in loaded_ctr:\n",
    "            print('#########################################')\n",
    "            \n",
    "            inc_ctr = lc + '_inc'\n",
    "            inc_path = path + inc_ctr\n",
    "            \n",
    "            bl, at, nd = create_fold(lc,inc_path)\n",
    "            \n",
    "            Black_list, attach, not_dw = Black_list + bl, attach + at, not_dw + nd\n",
    "\n",
    "            # Si tenes no se descargaron todos los archivos borro la carpeta\n",
    "            if bl == []:\n",
    "                os.system('mv \"'+path+inc_ctr+'\" \"'+path+lc+'\"')\n",
    "            else:\n",
    "                os.system('rm -rf \"'+path+inc_ctr+'\"')\n",
    "                \n",
    "            print('#########################################\\n\\n')\n",
    "            \n",
    "    return Black_list, attach, not_dw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f04af897-bf3b-4630-b30c-8998cd14ed96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################\n",
      "Venezuela (Bolivarian Republic of): \n",
      "                                  -Legislation 100%                                                                                                                                                               \n",
      "                                  -Strategies 100%                     \n",
      "                                  -Treaties 100%                                                                                                                                                                \n",
      "#########################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path='/home/kenneths/SherlockData/'\n",
    "bl, at, nd = download_lat_files(path,lat_ctrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afe75d6-deef-4749-b617-c4484d672f46",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Individual proving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "51f809ad-c1df-48d3-ab4c-c0459a5e58a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url='https://sherloc.unodc.org/cld//legislation/cri/ley_no._7.317_ley_de_conservacion_de_la_vida_silvestre/capitulo_iv/articulo_14-27/capitulo_iv.html?lng=en&amp;tmpl=sherloc'\n",
    "page_title = 'Ley No. 7.317 Ley de Conservación de la Vida SilvestreCapitulo IV: Conservación y manejo de la vida silvestre Articulo 14-27'\n",
    "\n",
    "fold_title='test'\n",
    "path='/home/kenneths/SherlockData/test'\n",
    "text,at=load_text(url)\n",
    "latex_text=generate_latex_text(page_title,text)\n",
    "bl,msg = create_file_system(path, fold_title, latex_text, page_title, url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
